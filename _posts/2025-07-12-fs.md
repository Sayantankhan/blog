---
layout: post
title: "Unlocking Extreme I/O: A Deep Dive into Modern Filesystem Architectures"
date: 2025-07-12
tags: ["filesystem", "Fuse", "DAOS", "HPC", "Deepseek - 3fs"]
---
In the ever-evolving world of AI, HPC, and large-scale distributed systems, how we read, write, and move data can make or break application performance. Traditional filesystems are often not enough ‚Äî which is where modern, user-space and high-performance distributed filesystems step in. This blog demystifies some of the most powerful options available today ‚Äî from FUSE's flexibility, to DAOS's object-level speed, to DeepSeek 3FS's AI-optimized throughput.

---
## FUSE (Filesystem in Userspace)

FUSE (Filesystem in Userspace) is a Linux kernel framework that lets user-space processes implement filesystems. It consists of a kernel module (`fuse.ko`), a user-space library (`libfuse`), and utilities like fusermount. FUSE allows non-privileged users to mount filesystems, such as remote directories over SSH (via `SSHFS`), encrypted containers, or object storage mounts like S3FS.

Unlike traditional in-kernel filesystems, FUSE-based filesystems run entirely in user space. This flexibility is useful for rapid prototyping, network-backed filesystems, or use cases where kernel development isn't practical. However, this comes at a cost: FUSE incurs noticeable overhead in both CPU and context switching. Studies (e.g., USENIX) have shown throughput drops of up to ~83% and CPU usage increases of ‚âà31% compared to native filesystems depending on workload.

FUSE is widely used in projects like SSHFS, EncFS, S3FS, and NTFS-3G.

### üßµ Concurrency and Multi-threading

FUSE supports multi-threaded operation with libfuse3, enabling concurrent handling of multiple filesystem operations. Developers can implement multi-threaded handlers in user space, while the kernel queues requests concurrently, improving throughput under load.

However, FUSE's multi-threading implementation has architectural bottlenecks. I/O requests initiated by applications are placed into a shared multi-threaded queue in the kernel, guarded by a spin lock. The user-space FUSE daemon then pulls from this queue to process requests. As concurrency increases, lock contention on this shared queue intensifies, preventing FUSE from scaling I/O linearly with threads. Performance benchmarks show FUSE saturates at ~400K 4KiB reads/sec ‚Äî beyond which performance plateaus or even degrades due to the spin lock overhead.

`perf` profiling shows a substantial amount of CPU cycles are spent managing this contention, limiting its usability in high-throughput environments like AI or data analytics.

Moreover, Linux 5.x FUSE does not support concurrent writes to the same file. Applications often work around this by using separate files or aggregating writes in memory before flushing, which complicates programming and impacts I/O scheduling.

Read patterns, particularly in machine learning workloads, add further stress: data loaders performing small, non-aligned random reads (few KB to MB) see poor performance over FUSE mounts. The combination of lock contention, memory copy overhead, and lack of read alignment support prevents full utilization of high-speed devices like SSDs or RDMA-backed networks.

### ‚öôÔ∏è Setup
```bash
sudo apt update
sudo apt install fuse3 libfuse2
```

Ensure your user is in the `fuse` group:
```bash
sudo usermod -aG fuse $USER
```

Edit `/etc/fuse.conf` and uncomment: user_allow_other

üõ†Ô∏è Mounting Example: SSHFS 
```bash
sudo apt install sshfs
mkdir ~/mnt_remote
sshfs user@host:/remote/path ~/mnt_remote -o allow_other
fusermount3 -u ~/mnt_remote
```

### üöÄ Performance Notes

#### Asynchronous Zero-Copy API Evolution

While implementing file system clients as VFS kernel modules can avoid many performance pitfalls, kernel development remains complex and risky. A buggy kernel module can crash systems with no logs, and upgrading requires careful process shutdowns or full system reboots.

To balance performance with safety, modern implementations embed native clients within the FUSE daemon. These clients expose an asynchronous, zero-copy API inspired by io_uring. Here's how it works:

`Iov`: A large, shared memory buffer used for direct RDMA-accessible reads/writes. All reads are written into the Iov buffer, and all writes must be staged there.

`Ior`: A small shared ring buffer that queues I/O requests. It resembles io_uring, enabling the user process to enqueue ops and native clients to process them in batches.

Multiple `Ior` rings can be used in parallel (recommended for multithreaded apps) to avoid lock contention. Within the native client, multiple threads pull requests from these rings and dispatch batched I/O operations to the backend storage, significantly reducing RPC pressure.

This model boosts performance dramatically for large sequential reads/writes and aligns well with AI workloads that buffer and flush I/O in bulk.
- Up to 80% lower throughput vs native FS in metadata-heavy ops
- Context switch overhead
- Multi-threaded in `libfuse3`
- Useful for experimental/dev/custom FS

## DeepSeek 3FS (Fire-Flyer File System)
### üîç Why DeepSeek 3FS and How It Differs from FUSE
FUSE enables rapid development of flexible filesystems, but falls short for I/O-intensive AI workloads due to its user-space bottlenecks. DeepSeek 3FS (also known as Fire-Flyer FS or 3FS) is purpose-built to overcome these limitations.

FUSE enables rapid development of flexible filesystems, but falls short for I/O-intensive AI workloads due to its user-space bottlenecks. DeepSeek 3FS (also known as Fire-Flyer FS or 3FS) is purpose-built to overcome these limitations.

Each 3FS storage service manages a few local SSDs and presents a chunk-store interface. Data is broken into fixed-size chunks and distributed across the cluster. These chunks are replicated across multiple SSDs using Chain Replication with Apportioned Queries (CRAQ) ‚Äî a protocol that ensures strong consistency while minimizing write latency. CRAQ‚Äôs write-all-read-any model helps saturate both NVMe and RDMA bandwidths, making full use of modern high-performance hardware. The chunk‚ÄØengine enables writes of 10+‚ÄØGiB/s per node, supporting fast checkpointing and data loading for AI workloads

```code
[Client] ‚áÑ [Storage Service: Chunk-Store + CRAQ on SSDs] ‚áÑ RDMA
                  ‚Üë
                  Metadata + FoundationDB
```                  

To prevent RDMA incast, the storage service coordinates read requests by asking clients for permission before sending data. This ensures sustainable high throughput, even on fat-tree networks

### üß© File metadata store
3FS splits file data into equal-sized chunks and stripes them across multiple replication chains. Users can pick chain tables, chunk size, and stripe size per directory. Each chunk gets a unique ID‚Äîbuilt by combining the file's inode ID and its chunk index.

**How it works:**
1. On file creation, the metadata service selects consecutive chains (based on stripe size), then shuffles them using a randomized seed‚Äîto ensure balanced data distribution.
2. Files are accessed independently by clients: once metadata is fetched, chunk IDs and locations are computed locally‚Äîminimizing metadata service involvement in hot I/O paths.

#### **Metadata Management via FoundationDB**
3FS employs stateless metadata services underpinned by FoundationDB, leveraging its transactional key-value store (SSI). All metadata‚Äîlike inodes and directory entries‚Äîare stored as FDB key-values.

- **Inode keys** (`INOD<inode>`, little-endian) store attributes, chunk layout, stripe settings, and shuffle seed.
- **Directory entry keys** (`DENT<parent_inode><filename>`) map names to inode IDs and types.

Transactional operations:
- Read-only for `stat`, `lookup`, `readdir`
- Read-write for `create`, `rename`, `unlink`

This design supports concurrent operations, conflict detection, and automatic retries. The stateless service model simplifies deployment and enables in-service upgrades or failover without disruption. When clients fail, metadata services rebalance seamlessly‚Äîpreserving consistency without complex choreography.  

### üì¶ Chunk storage system and Data Placement
The 3FS chunk storage system is engineered to maximize I/O throughput‚Äîeven during SSD failures‚Äîby scaling linearly with the number of SSDs and network bisection bandwidth. It treats applications as locally oblivious, meaning clients access storage without concern for physical data placement

#### üß± Chain Replication with CRAQ
- Files are divided into fixed-size chunks and mapped onto chains of storage targets using Chain Replication with Apportioned Queries (CRAQ)

- Each chunk is replicated across a head, middle, and tail target. Writes go through the chain; reads can come from any replica‚Äîincreasing parallelism and load balance

![Chain Replication Model](/assets/img/Chain_replication.png)

Example setup:
 - Six nodes ùê¥‚Äìùêπ, each with 1 SSD holding 5 targets (A1‚ÄìA5, B1‚ÄìB5, ...)
 - Chains of length 3 (replica count):
 
 | Chain |    Targets   |
 | :---: | :----------: |
 |   1   | A1 ‚Üí B1 ‚Üí C1 |
 |   2   | D1 ‚Üí E1 ‚Üí F1 |
 |  ...  |      ...     |
 |   10  | D5 ‚Üí E5 ‚Üí F5 |

![Chain Replication Model](/assets/img/Data_Placement_chain.png)


### ‚öñÔ∏è Balanced Recovery Traffic
If node A fails, default CRAQ behavior redirects all its replicas‚Äô reads to its partners (e.g., B1, C1), potentially creating hotspots. To avoid this, 3FS employs balanced chain tables to distribute redirected reads across multiple nodes like B‚ÄìF, using combinatorial block design algorithms for optimal traffic balance

### CRAQ Data Replication Protocol

#### üõ†Ô∏è Write Path:
1. Verify chain version matches current.
2. Pull write data via RDMA.
3. Acquire chunk lock (serializes writes at head).
4. Create a pending version of the chunk.
5. If at tail: commit the pending version and send ACK upstream.
6. Otherwise: forward the write to next node.
7. On ACK, each node commits and releases locks 
8. if a chain node fails mid-write, manager updates chain, head retries the write to new successor .

#### Read Path:
1. If only committed version is present, serve it.
2. If both committed and pending exist, return a special code prompting the client to retry or request the pending version

## DAOS - Distributed Asynchronous Object Storage

DAOS (Distributed Asynchronous Object Store) is a high-performance, scale-out object storage system built for modern hardware and AI/HPC workloads. Unlike FUSE or 3FS, it operates as a true user-space cluster backend, bypassing kernel limitations for maximum performance and scalability.

### üèóÔ∏è DAOS Core Architecture
- `Kernel-bypass I/O:` Uses PMDK for storage-class memory and SPDK for NVMe‚Äîeliminating syscall overhead and delivering microsecond-level latency (<20‚ÄØ¬µs) and millions of IOPS per node
- `Object Storage Model:` Organizes data in pools (across nodes/engines) and containers that hold versioned objects, arrays, or key-values 
- `Fabric-optimized Networking`: Built on OFI/UCX over RDMA or Ethernet‚Äîenabling zero-copy scatter-gather transfers
- `Epoch-based Transactional IO`: All operations are tagged with epochs and are consistently versioned via the Versioning Object Store (VOS) 
- `Redundancy and Consistency`: Supports N-way replication and erasure coding with auto-rebuild and online recovery features

### üîß libdfs Internals: POSIX Over DAOS

- `libdfs` implements POSIX APIs by mapping files/directories to DAOS objects, using arrays and dkey/akey construct
- While libdfs-level I/O bypasses kernel overhead (in API mode), metadata operations (e.g. mkdir, open) still follow DFS semantics
- Direct API calls via `libdaos` (using DFS or native object ops) offer significantly lower overhead compared to FUSE-mounted namespace

### üõ†Ô∏è DFuse Tuning: Getting the Most from POSIX
**DFuse Daemon**: Typically runs one per node, spawning one thread per CPU core by default. Its thread count can be configured via --thread-count, while numactl or taskset can bind threads per NUMA/socket 

**Caching Control:** 
- Data and metadata caching are enabled by default.
- Use --disable-caching or set container attributes (dfuse-data-cache, dfuse-attr-time, etc.) for strict consistency 

**Interception via libioil:**
- LD_PRELOAD of libioil redirects read/write syscalls to bypass FUSE and go directly through DAOS APIs.
- Tests show data path throughput improves from ~1.5‚ÄØGB/s (FUSE) to ~4‚ÄØGB/s (libioil) for large I/O 

**Multi-user and Shared Modes**: Use --multi-user for shared environments, ensuring permissions and UIDs are properly enforced .

### üõ°Ô∏è Fault Tolerance & High Availability
**Hierarchical Fault Domains:** Pools are configured across failure boundaries (node/rack) to ensure high resilience .

**Failure Detected via SWIM**: Storage targets and engines use SWIM protocol to detect failures and rebalance membership .

**Automatic Rebuilds:** Upon detecting failures, faulty targets are removed and system upgrades its ‚Äúpool map‚Äù. Clients and servers adapt transparently to topology changes

**Redundancy Controls**:
- N-way replication allows tolerance for N-1 failures.
- Erasure codes (e.g., EC_16P2) provide efficient data protection with minimal space overhead 

**Lockless RPC Resilience:** Clients include pool map versions in RPCs; servers detect mismatches, and reconnect automatically to new configurations during updates

| Area                  | DAOS Details                                                           |
| --------------------- | ---------------------------------------------------------------------- |
| I/O Path              | Kernel bypass via PMDK/SPDK                                            |
| Metadata API          | POSIX via DFS & libdfs; RPC via libdaos                                |
| Data Path             | Bypasses kernel with libioil or DFS-native use                         |
| Fault Tolerance       | SWIM-based failure detection; automatic rebuilds                       |
| Redundancy Strategies | Replication & erasure coding (container-level)                         |
| Performance Impact    | Extreme I/O (>GB/s/node), uS latency, high concurrency under RDMA/NVMe |


#### üõ°Ô∏è SWIM-Based Failure Detection

DAOS uses a **SWIM (Scalable Weakly-consistent Infection-style Membership)** protocol to detect and isolate node or engine failures efficiently:

- **Gossip-style probing**: Each server periodically sends randomized pings to others. If a ping times out, it initiates indirect probes to confirm liveness.
- **Suspicion and confirmation**: Nodes start as *suspected* before being marked *failed*, enabling false-positive reduction through secondary checks.
- **Epidemic dissemination**: Detecting a failure leads to gossip-based updates, propagating membership changes in **O(log‚ÄØN)** time‚Äîkeeping per-node message overhead constant regardless of cluster size.
- **Automatic exclusion**: Once a node is confirmed as unresponsive, DAOS marks it excluded in the pool map and initiates rebuilds or reroutes to maintain data availability and integrity :contentReference[oaicite:1]{index=1}.


## üåü Conclusion
- `FUSE` offers unmatched flexibility and ease of development, making it excellent for custom or networked filesystems. However, it incurs significant CPU and latency penalties, especially under concurrent workloads.

- `DeepSeek 3FS (Fire‚ÄëFlyer FS)` bridges this gap by bringing distributed storage, chunk-striping, and CRAQ-based consistency to user-space filesystems. Its architecture delivers multi-TiB/s performance and fault tolerance, ideal for AI/ML data pipelines.

- `DAOS` goes even further by embracing a kernel-bypass, object-storage model built around NVMe, SCM, and RDMA. With support for POSIX (libdfs/DFuse), MPI, or native I/O, DAOS offers microsecond latency, linear scalability, and strong resilience features‚Äîmaking it a powerful backend for HPC, AI, and analytics.

Each filesystem strikes a different balance between flexibility, performance, and scale. By understanding their trade-offs and tuning them appropriately, we can unlock both productivity and extreme I/O throughput

## üìö References

- [FUSE Libfuse Docs](https://github.com/libfuse/libfuse) - official Documentation 
- `USENIX` study on FUSE overhead and context-switch bottlenecks
- [DAOS Official](https://docs.daos.io/) - SWIM membership details, target exclusion, pool rebuild logic, engine shutdown workflows.
- [3FS GitHub](https://github.com/deepseek-ai/3fs) -  official README and architecture codebase detailing
- [3FS Deep Dive](https://maknee.github.io/blog/2025/3FS-Performance-Journal-1/?utm_source=chatgpt.com) - covers CRAQ behavior and chain replication, showing head/tail mechanics and how strong consistency is ensured.
- [3FS vs. JuiceFS](https://juicefs.medium.com/deepseek-3fs-vs-juicefs-architectures-features-and-innovations-in-ai-storage-628af5f189e7) - highlights metadata service, cluster manager, and FUSE/native client differences. 
- [Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning](https://arxiv.org/abs/2408.14158)
